{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/avinashc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/avinashc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Import all required models and flows ####\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack, csr_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#readability\n",
    "import textstat\n",
    "\n",
    "#sytax\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helper functions used later ####\n",
    "def cleanArticle(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9' ]+\")\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "#### Read the dataset into a dataframe for further processing ####\n",
    "def read_dataset(dataset_name):\n",
    "    \n",
    "    def remove_numbers(in_str):\n",
    "        return re.sub(r'[0-9]+', '', in_str)\n",
    "    \n",
    "    print(\"Reading dataset\")\n",
    "    result_data_list = []\n",
    "    data_dir = DATASET_PATH\n",
    "    for news_type in ['fake', 'legit']:\n",
    "        folder = '%s/%s/%s' % (data_dir, dataset_name, news_type)\n",
    "        for fname in os.listdir(folder):\n",
    "            result_data = {}\n",
    "            result_data['dataset_name'] = dataset_name\n",
    "            result_data['news_type'] = news_type\n",
    "            if news_type == 'fake':\n",
    "                result_data['is_fake'] = 1\n",
    "            else:\n",
    "                result_data['is_fake'] = 0\n",
    "            if dataset_name == 'fakeNewsDataset':\n",
    "                result_data['news_category'] = remove_numbers(fname.split('.')[0])\n",
    "            result_data['file_name'] = fname\n",
    "            filepath = os.path.join(folder, fname)\n",
    "            with open(filepath, 'r', encoding=\"utf8\") as f:\n",
    "                file_data = f.read().split('\\n')\n",
    "                # Some articles don't have a headline, but only article body.\n",
    "                if len(file_data) > 1:\n",
    "                    news_content_data = ' '.join(file_data[2:])\n",
    "                    result_data['news_headline'] = file_data[0]\n",
    "                else:\n",
    "                    news_content_data = file_data[0]\n",
    "                    result_data['news_headline'] = ''\n",
    "                result_data['news_content'] = news_content_data\n",
    "                result_data['news_all'] = ' '.join(file_data[0:])\n",
    "                result_data_list.append(result_data)\n",
    "                \n",
    "    df = pd.DataFrame(result_data_list)\n",
    "    \n",
    "    df['news_all_clean'] = df['news_all'].apply(lambda a: cleanArticle(a))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(['is_fake','news_type','file_name'],axis = 1), \n",
    "                                                        df['is_fake'], \n",
    "                                                        test_size=.2, random_state=RANDOM_SEED)\n",
    "    \n",
    "    print(\"Finished reading dataset\")\n",
    "    return df, X_train.reset_index(drop=True),\\\n",
    "            y_train.reset_index(drop=True), X_test.reset_index(drop=True), y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linguistic features helper function: Punctuation & Ngram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helper functions used later ####\n",
    "def pad_punct(s):\n",
    "    #Add padding around specified punctuation\n",
    "    s = re.sub('([.,!?():])', r' \\1 ', s)\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    return s\n",
    "\n",
    "def tfidf_vectorizer_custom(train, test, ngram_range):\n",
    "    #Create a tfidf vectorized set for train and test data that counts punctuation\n",
    "    train = train.apply(pad_punct)\n",
    "    test = test.apply(pad_punct)\n",
    "    vect = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\.|,|\\)|\\(|\\:|\\?|\\\"|\\'\", #pattern keep punctuation in vectorizer\n",
    "                          ngram_range = ngram_range).fit(train)\n",
    "    vocab = vect.vocabulary_.keys()\n",
    "    vocab = sorted(vocab, key=len)\n",
    "    print(\"Training data info:\")\n",
    "    print('- Vocabulary len:', len(vect.get_feature_names()))\n",
    "    print('- Longest phrase:', max(vect.vocabulary_, key=len))\n",
    "    print('- Smallest 10 phrases:', vocab[0:10])\n",
    "    print('- Sample of features:',np.array(vect.get_feature_names()))\n",
    "    train_vectorized = vect.transform(train)\n",
    "    test_vectorized = vect.transform(test)\n",
    "    return train_vectorized, test_vectorized, vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linguistic features helper function: Readability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_metric(df1):\n",
    "    \"\"\"\n",
    "    Input: a dataframe with column \"news_all\" to be analyzed for a variety of readbility metrics.\n",
    "    Output: the inputted dataframe, enriched with all readbility metrics \n",
    "            there are a total of 10, one of which is categorical\n",
    "    \"\"\"\n",
    "    df = df1.copy()\n",
    "    df_drop = df1.copy()\n",
    "    df[\"flesch_reading_ease\"]=0.0\n",
    "    df[\"smog_index\"]=0.0\n",
    "    df[\"flesch_kincaid_grade\"]=0.0\n",
    "    df[\"coleman_liau_index\"]=0.0\n",
    "    df[\"automated_readability_index\"]=0.0\n",
    "    df[\"dale_chall_readability_score\"]=0.0\n",
    "    df[\"difficult_words\"]=0.0\n",
    "    df[\"linsear_write_formula\"]=0.0\n",
    "    df[\"gunning_fog\"]=0.0\n",
    "    df[\"text_standard\"]=0.0\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        text = df['news_all'].iloc[i]\n",
    "        df.loc[i,(\"flesch_reading_ease\")]= textstat.flesch_reading_ease(text)\n",
    "        df.loc[i,(\"smog_index\")]= textstat.smog_index(text)\n",
    "        df.loc[i,(\"flesch_kincaid_grade\")]= textstat.flesch_kincaid_grade(text)\n",
    "        df.loc[i,(\"coleman_liau_index\")]= textstat.coleman_liau_index(text)\n",
    "        df.loc[i,(\"automated_readability_index\")]= textstat.automated_readability_index(text)\n",
    "        df.loc[i,(\"dale_chall_readability_score\")]= textstat.dale_chall_readability_score(text)\n",
    "        df.loc[i,(\"difficult_words\")]= textstat.difficult_words(text)\n",
    "        df.loc[i,(\"linsear_write_formula\")]= textstat.linsear_write_formula(text)\n",
    "        df.loc[i,(\"gunning_fog\")]= textstat.gunning_fog(text)\n",
    "        df.loc[i,(\"text_standard\")]= textstat.text_standard(text)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linguistic features helper function: Syntax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagger(text):\n",
    "    # Tag POS using nltk. Return a single \"sentence\" with the POS of each word treated like a \"word.\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text = nltk.Text(tokens)\n",
    "    tags = nltk.pos_tag(text)\n",
    "    return \" \".join([i[1] for i in tags])\n",
    "\n",
    "def POS_enricher(df1):\n",
    "    \"\"\"\n",
    "    Input: a dataframe with column \"news_headline\" and \"news_content\".\n",
    "    Output: the inputted dataframe, with the columns \"news_headline_POS\" and \"news_content_POS\" created \n",
    "            which are a single sentence with the POS of each word represented as word in that same position.\n",
    "    \"\"\"\n",
    "    df = df1.copy()\n",
    "    df[\"news_headline_POS\"]=\"\"\n",
    "    df[\"news_content_POS\"]=\"\"\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        text_headline = cleanArticle(POS_tagger(df['news_headline'].apply(lambda a: cleanArticle(a)).iloc[i]))\n",
    "        text_content = cleanArticle(POS_tagger(df['news_content'].apply(lambda a: cleanArticle(a)).iloc[i]))\n",
    "        \n",
    "        df.loc[i,(\"news_headline_POS\")]= text_headline\n",
    "        df.loc[i,(\"news_content_POS\")]= text_content\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Finished reading dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>news_category</th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_content</th>\n",
       "      <th>news_all</th>\n",
       "      <th>news_all_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>edu</td>\n",
       "      <td>Donald Trump's Win To The Presidency Causes Fr...</td>\n",
       "      <td>High school students spend years being taugh...</td>\n",
       "      <td>Donald Trump's Win To The Presidency Causes Fr...</td>\n",
       "      <td>donald trump's win to the presidency causes fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>edu</td>\n",
       "      <td>Trump's Pick for Education Could Face Unusuall...</td>\n",
       "      <td>Nominees for secretary of education have typic...</td>\n",
       "      <td>Trump's Pick for Education Could Face Unusuall...</td>\n",
       "      <td>trump's pick for education could face unusuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>biz</td>\n",
       "      <td>Macron and Le Pen Fight for Votes</td>\n",
       "      <td>Anticipating French Election Runoff \"With more...</td>\n",
       "      <td>Macron and Le Pen Fight for Votes  Anticipatin...</td>\n",
       "      <td>macron and le pen fight for votes  anticipatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>edu</td>\n",
       "      <td>Girls Who Code Closing Computer Science Gender...</td>\n",
       "      <td>It's time to close the gender gap. One of the...</td>\n",
       "      <td>Girls Who Code Closing Computer Science Gender...</td>\n",
       "      <td>girls who code closing computer science gender...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>fakeNewsDataset</td>\n",
       "      <td>polit</td>\n",
       "      <td></td>\n",
       "      <td>The U.S. Supreme court has done the unthinkabl...</td>\n",
       "      <td>The U.S. Supreme court has done the unthinkabl...</td>\n",
       "      <td>the us supreme court has done the unthinkable ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset_name news_category  \\\n",
       "0  fakeNewsDataset           edu   \n",
       "1  fakeNewsDataset           edu   \n",
       "2  fakeNewsDataset           biz   \n",
       "3  fakeNewsDataset           edu   \n",
       "4  fakeNewsDataset         polit   \n",
       "\n",
       "                                       news_headline  \\\n",
       "0  Donald Trump's Win To The Presidency Causes Fr...   \n",
       "1  Trump's Pick for Education Could Face Unusuall...   \n",
       "2                  Macron and Le Pen Fight for Votes   \n",
       "3  Girls Who Code Closing Computer Science Gender...   \n",
       "4                                                      \n",
       "\n",
       "                                        news_content  \\\n",
       "0    High school students spend years being taugh...   \n",
       "1  Nominees for secretary of education have typic...   \n",
       "2  Anticipating French Election Runoff \"With more...   \n",
       "3   It's time to close the gender gap. One of the...   \n",
       "4  The U.S. Supreme court has done the unthinkabl...   \n",
       "\n",
       "                                            news_all  \\\n",
       "0  Donald Trump's Win To The Presidency Causes Fr...   \n",
       "1  Trump's Pick for Education Could Face Unusuall...   \n",
       "2  Macron and Le Pen Fight for Votes  Anticipatin...   \n",
       "3  Girls Who Code Closing Computer Science Gender...   \n",
       "4  The U.S. Supreme court has done the unthinkabl...   \n",
       "\n",
       "                                      news_all_clean  \n",
       "0  donald trump's win to the presidency causes fr...  \n",
       "1  trump's pick for education could face unusuall...  \n",
       "2  macron and le pen fight for votes  anticipatin...  \n",
       "3  girls who code closing computer science gender...  \n",
       "4  the us supreme court has done the unthinkable ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dataset area\n",
    "RANDOM_SEED = 139\n",
    "DATASET_PATH = \"fakeNewsDatasets\"\n",
    "ID_UNKNOWN = 399999\n",
    "\n",
    "news_full, news_train_data, news_train_labels, news_test_data, news_test_labels = read_dataset('fakeNewsDataset')\n",
    "news_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Finished reading dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>news_headline</th>\n",
       "      <th>news_content</th>\n",
       "      <th>news_all</th>\n",
       "      <th>news_all_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>celebrityDataset</td>\n",
       "      <td>Lady Gaga Announces Netflix Documentary, Talks...</td>\n",
       "      <td>Lady Gaga is ready to open up. The “Joanne” si...</td>\n",
       "      <td>Lady Gaga Announces Netflix Documentary, Talks...</td>\n",
       "      <td>lady gaga announces netflix documentary talks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>celebrityDataset</td>\n",
       "      <td>This Is What Brad Pitt Has Been Texting Jennif...</td>\n",
       "      <td>It has been six months since Brangelina (that'...</td>\n",
       "      <td>This Is What Brad Pitt Has Been Texting Jennif...</td>\n",
       "      <td>this is what brad pitt has been texting jennif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>celebrityDataset</td>\n",
       "      <td>Caitlyn Jenner Looks to New Girlfriend for Sup...</td>\n",
       "      <td>The wrath of the Kardashians is the least of C...</td>\n",
       "      <td>Caitlyn Jenner Looks to New Girlfriend for Sup...</td>\n",
       "      <td>caitlyn jenner looks to new girlfriend for sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>celebrityDataset</td>\n",
       "      <td>'Girl' Fight! Kanye West Confronts Anna Wintou...</td>\n",
       "      <td>Too big for your designed sneakers, Kanye? Dur...</td>\n",
       "      <td>'Girl' Fight! Kanye West Confronts Anna Wintou...</td>\n",
       "      <td>'girl' fight kanye west confronts anna wintour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>celebrityDataset</td>\n",
       "      <td>Caitlyn Jenner's Memoir Reportedly Lands Movie...</td>\n",
       "      <td>Report says that the 'How to Be a Latin Lover'...</td>\n",
       "      <td>Caitlyn Jenner's Memoir Reportedly Lands Movie...</td>\n",
       "      <td>caitlyn jenner's memoir reportedly lands movie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset_name                                      news_headline  \\\n",
       "0  celebrityDataset  Lady Gaga Announces Netflix Documentary, Talks...   \n",
       "1  celebrityDataset  This Is What Brad Pitt Has Been Texting Jennif...   \n",
       "2  celebrityDataset  Caitlyn Jenner Looks to New Girlfriend for Sup...   \n",
       "3  celebrityDataset  'Girl' Fight! Kanye West Confronts Anna Wintou...   \n",
       "4  celebrityDataset  Caitlyn Jenner's Memoir Reportedly Lands Movie...   \n",
       "\n",
       "                                        news_content  \\\n",
       "0  Lady Gaga is ready to open up. The “Joanne” si...   \n",
       "1  It has been six months since Brangelina (that'...   \n",
       "2  The wrath of the Kardashians is the least of C...   \n",
       "3  Too big for your designed sneakers, Kanye? Dur...   \n",
       "4  Report says that the 'How to Be a Latin Lover'...   \n",
       "\n",
       "                                            news_all  \\\n",
       "0  Lady Gaga Announces Netflix Documentary, Talks...   \n",
       "1  This Is What Brad Pitt Has Been Texting Jennif...   \n",
       "2  Caitlyn Jenner Looks to New Girlfriend for Sup...   \n",
       "3  'Girl' Fight! Kanye West Confronts Anna Wintou...   \n",
       "4  Caitlyn Jenner's Memoir Reportedly Lands Movie...   \n",
       "\n",
       "                                      news_all_clean  \n",
       "0  lady gaga announces netflix documentary talks ...  \n",
       "1  this is what brad pitt has been texting jennif...  \n",
       "2  caitlyn jenner looks to new girlfriend for sup...  \n",
       "3  'girl' fight kanye west confronts anna wintour...  \n",
       "4  caitlyn jenner's memoir reportedly lands movie...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeb_full, celeb_train_data, celeb_train_labels, celeb_test_data, celeb_test_labels = read_dataset('celebrityDataset')\n",
    "celeb_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new Punctuation and Ngram features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data info:\n",
      "- Vocabulary len: 39662\n",
      "- Longest phrase: fundamental misunderstanding\n",
      "- Smallest 10 phrases: [\"'\", ',', '.', '\"', ':', '!', '?', '(', ')', 'to']\n",
      "- Sample of features: ['!' '! !' '! \"' ... 'zuckerberg ,' 'zverev' 'zverev roger']\n",
      "train shape: (384, 39662)\n"
     ]
    }
   ],
   "source": [
    "#1. FakeNews\n",
    "news_train_data_vectorized, news_test_data_vectorized, news_ngram_punct_names = tfidf_vectorizer_custom(\n",
    "    news_train_data['news_all'], news_test_data['news_all'], ngram_range = (1,2))\n",
    "print(\"train shape:\", news_train_data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data info:\n",
      "- Vocabulary len: 98506\n",
      "- Longest phrase: 3333333333333333333333333333333333333 triplets\n",
      "- Smallest 10 phrases: [',', '.', ':', '?', '(', \"'\", ')', '!', '\"', 'is']\n",
      "- Sample of features: ['!' '! !' '! \"' ... 'zucker says' 'zwang' 'zwang ,']\n",
      "train shape: (400, 98506)\n"
     ]
    }
   ],
   "source": [
    "#2. Celeb\n",
    "celeb_train_data_vectorized, celeb_test_data_vectorized, celeb_ngram_punct_names = tfidf_vectorizer_custom(\n",
    "    celeb_train_data['news_all'], celeb_test_data['news_all'], ngram_range = (1,2))\n",
    "print(\"train shape:\", celeb_train_data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new Readability features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (384, 16)\n"
     ]
    }
   ],
   "source": [
    "#1. FakeNews\n",
    "news_train_data_readability = readability_metric(news_train_data)\n",
    "news_test_data_readability = readability_metric(news_test_data)\n",
    "print(\"train shape:\", news_train_data_readability.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (400, 15)\n"
     ]
    }
   ],
   "source": [
    "#2. Celb\n",
    "celeb_train_data_readability = readability_metric(celeb_train_data)\n",
    "celeb_test_data_readability = readability_metric(celeb_test_data)\n",
    "print(\"train shape:\", celeb_train_data_readability.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new Syntax features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data info:\n",
      "- Vocabulary len: 1176\n",
      "- Longest phrase: nns vbp pos\n",
      "- Smallest 10 phrases: [\"'\", 'jj', 'nn', 'to', 'dt', 'vb', 'rb', 'in', 'md', 'cc']\n",
      "- Sample of features: [\"'\" \"' '\" \"' ' in\" ... 'wrb rb vbz' 'wrb vbg' 'wrb vbg vbp']\n",
      "Training data info:\n",
      "- Vocabulary len: 4459\n",
      "- Longest phrase: nns vbp nns\n",
      "- Smallest 10 phrases: [\"'\", 'jj', 'nn', 'in', 'cc', 'md', 'rb', 'vb', 'dt', 'to']\n",
      "- Sample of features: [\"'\" \"' '\" \"' ' in\" ... 'wrb vbp vb' 'wrb vbz' 'wrb vbz cd']\n",
      "Training data info:\n",
      "- Vocabulary len: 1589\n",
      "- Longest phrase: vbz vbn vbg\n",
      "- Smallest 10 phrases: [\"'\", 'jj', 'nn', 'dt', 'wp', 'to', 'in', 'cd', 'rb', 'md']\n",
      "- Sample of features: [\"'\" \"' '\" \"' ' cc\" ... 'wrb to vb' 'wrb vbd' 'wrb vbd vbn']\n",
      "Training data info:\n",
      "- Vocabulary len: 7831\n",
      "- Longest phrase: vbg prp nns\n",
      "- Smallest 10 phrases: [\"'\", 'nn', 'jj', 'to', 'vb', 'rp', 'dt', 'cd', 'in', 'rb']\n",
      "- Sample of features: [\"'\" \"' '\" \"' ' cc\" ... 'wrb vbz vbn' 'wrb wrb' 'wrb wrb prp']\n"
     ]
    }
   ],
   "source": [
    "def syntax_tfidf(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Input: Training and Testing set\n",
    "    Ouptu: Enriched sets with tfidf of POS tags for text in news_content and news_headline.\n",
    "    \"\"\"\n",
    "    train_POS_init = POS_enricher(train_data)\n",
    "    test_POS_init = POS_enricher(test_data)\n",
    "\n",
    "    headline_train_data, headline_test_data, _ = tfidf_vectorizer_custom(\n",
    "        train_POS_init['news_headline_POS'], test_POS_init['news_headline_POS'], ngram_range = (1,3))\n",
    "\n",
    "    content_train_data, content_test_data, _ = tfidf_vectorizer_custom(\n",
    "        train_POS_init['news_content_POS'], test_POS_init['news_content_POS'], ngram_range = (1,3))\n",
    "\n",
    "    train_POS = hstack([headline_train_data, content_train_data])\n",
    "    test_POS = hstack([headline_test_data, content_test_data])\n",
    "    \n",
    "    return train_POS, test_POS\n",
    "\n",
    "#1. FakeNews\n",
    "news_train_POS, news_test_POS = syntax_tfidf(news_train_data, news_test_data)\n",
    "\n",
    "#2. Celeb\n",
    "celeb_train_POS, celeb_test_POS = syntax_tfidf(celeb_train_data, celeb_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helper functions to add all features together ####\n",
    "def add_feature(sparse, df_to_sparse, df_ignore):\n",
    "    # Returns sparse feature matrix with added feature.\n",
    "    return hstack([coo_matrix(df_to_sparse.drop(df_ignore.columns.values, axis = 1)),sparse])\n",
    "\n",
    "def add_categorical_feature(train_sparse, test_sparse, train_data, test_data, categorical_column, collapse = True):\n",
    "    # Returns sparse feature matrix with added feature by creating a countvectorizer of feature.\n",
    "    veccat = CountVectorizer()\n",
    "    veccat.fit(train_data[categorical_column])\n",
    "    train_category_vec = veccat.transform(train_data[categorical_column])\n",
    "    test_category_vec  = veccat.transform(test_data[categorical_column])\n",
    "    train_final = hstack([train_sparse, train_category_vec])\n",
    "    test_final =  hstack([test_sparse, test_category_vec])\n",
    "    return train_final, test_final, veccat.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_train_ngram_read (384, 39671)\n",
      "Different categories are ['0th', '10th', '11th', '12th', '13th', '14th', '15th', '16th', '17th', '18th', '19th', '1th', '20th', '21st', '22nd', '23rd', '24th', '25th', '26th', '27th', '28th', '29th', '30th', '31st', '32nd', '34th', '35th', '36th', '37th', '38th', '39th', '40th', '41st', '43rd', '44th', '46th', '47th', '48th', '49th', '53rd', '54th', '55th', '56th', '57th', '60th', '61st', '6th', '7th', '8th', '9th', 'and', 'grade']\n",
      "news_train_ngram_read (384, 39723)\n"
     ]
    }
   ],
   "source": [
    "#1. Combine readability with ngram\n",
    "news_train_ngram_read = add_feature(news_train_data_vectorized, \n",
    "                                         news_train_data_readability.drop([\"text_standard\"],axis = 1), news_train_data)\n",
    "news_test_ngram_read = add_feature(news_test_data_vectorized, \n",
    "                                        news_test_data_readability.drop([\"text_standard\"],axis = 1), news_test_data)\n",
    "print(\"news_train_ngram_read\", news_train_ngram_read.shape)\n",
    "news_train_ngram_read, news_test_ngram_read, news_text_standard_names = add_categorical_feature(\n",
    "    news_train_ngram_read, news_test_ngram_read, news_train_data_readability, news_test_data_readability, \n",
    "    \"text_standard\")\n",
    "print(\"Different categories are\", news_text_standard_names)\n",
    "print(\"news_train_ngram_read\", news_train_ngram_read.shape)\n",
    "\n",
    "#2. Combine syntax with readability, ngram\n",
    "news_train_allfeats = hstack([news_train_ngram_read, news_train_POS])\n",
    "news_test_allfeats = hstack([news_test_ngram_read, news_test_POS])\n",
    "\n",
    "#3. Add news category\n",
    "news_train_final, news_test_final, news_news_category_names = add_categorical_feature(news_train_ngram_read, \n",
    "                                                                                      news_test_ngram_read, \n",
    "                                                  news_train_data, news_test_data, \"news_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_train_ngram_read (384, 39723)\n",
      "news_train_allfeats (384, 45358)\n",
      "news_train_final (384, 39729)\n"
     ]
    }
   ],
   "source": [
    "print(\"news_train_ngram_read\", news_train_ngram_read.shape)\n",
    "print(\"news_train_allfeats\", news_train_allfeats.shape)\n",
    "print(\"news_train_final\", news_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Combine readability with ngram\n",
    "celeb_train_ngram_read = add_feature(celeb_train_data_vectorized, \n",
    "                                         celeb_train_data_readability.drop([\"text_standard\"],axis = 1), celeb_train_data)\n",
    "celeb_test_ngram_read = add_feature(celeb_test_data_vectorized, \n",
    "                                        celeb_test_data_readability.drop([\"text_standard\"],axis = 1), celeb_test_data)\n",
    "celeb_train_ngram_read, celeb_test_ngram_read, celeb_text_standard_names = add_categorical_feature(\n",
    "    celeb_train_ngram_read, celeb_test_ngram_read, celeb_train_data_readability, celeb_test_data_readability, \n",
    "    \"text_standard\")\n",
    "\n",
    "#2. Combine syntax with readability, ngram\n",
    "celeb_train_allfeats = hstack([celeb_train_ngram_read, celeb_train_POS])\n",
    "celeb_test_allfeats = hstack([celeb_test_ngram_read, celeb_test_POS])\n",
    "\n",
    "celeb_train_final, celeb_test_final, celeb_dataset_names = add_categorical_feature(celeb_train_ngram_read, \n",
    "                                                                                   celeb_test_ngram_read, \n",
    "                                                                                   celeb_train_data, celeb_test_data,\n",
    "                                                                                   \"dataset_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celeb_train_ngram_read (400, 98581)\n",
      "celeb_train_allfeats (400, 108001)\n",
      "celeb_train_final (400, 98582)\n"
     ]
    }
   ],
   "source": [
    "print(\"celeb_train_ngram_read\", celeb_train_ngram_read.shape)\n",
    "print(\"celeb_train_allfeats\", celeb_train_allfeats.shape)\n",
    "print(\"celeb_train_final\", celeb_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on FakeNews test set:  0.7708333333333334\n",
      "Accuracy on Celebrity test set:  0.8003334\n"
     ]
    }
   ],
   "source": [
    "#1. FakeNews\n",
    "news_clf = RandomForestClassifier(bootstrap=True, criterion=\"gini\", \n",
    "                                           max_features=1.0, min_samples_leaf=15, \n",
    "                                           min_samples_split=6, n_estimators=300,\n",
    "                                          random_state = 1)\n",
    "\n",
    "news_clf.fit(news_train_final, news_train_labels)\n",
    "news_pred = news_clf.predict(news_test_final)\n",
    "news_proba = news_clf.predict_proba(news_test_final)\n",
    "print(\"Accuracy on FakeNews test set: \",np.mean(news_pred == news_test_labels))\n",
    "\n",
    "#2. Celeb\n",
    "celeb_clf = RandomForestClassifier(bootstrap=True, criterion=\"gini\", \n",
    "                                           max_features=0.05, min_samples_leaf=20, \n",
    "                                           min_samples_split=3, n_estimators=150,\n",
    "                                          random_state = 1)\n",
    "\n",
    "celeb_clf.fit(celeb_train_final, celeb_train_labels)\n",
    "celeb_pred = celeb_clf.predict(celeb_test_final)\n",
    "celeb_proba = celeb_clf.predict_proba(celeb_test_final)\n",
    "print(\"Accuracy on Celebrity test set: \",np.mean(celeb_pred == celeb_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAFgCAYAAAALlyh1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAfn0lEQVR4nO3deZwlZX3v8c9XQNxIEJjgimOUkOCGMoEgxmBALhKjmKDCVUTQIMb9XnGJhnAxXjUakyhGREXAKBKXMQSRRRRxhwEHBkSF4FwlEBk0rhAN8Lt/VDWeaU73nHnoc07PzOf9ep1X1/JUnV/1dM+3q07V86SqkCRJ6+cu0y5AkqQNkQEqSVIDA1SSpAYGqCRJDQxQSZIabD7tAhbSfvvtV2eddda0y5Ck9ZVpF6D1t1Gdgd54443TLkGStInYqAJUkqRJMUAlSWpggEqS1MAAlSSpgQEqSVIDA1SSpAYGqCRJDQxQSZIaGKCSJDUwQCVJamCASpLUwACVJKmBASpJUoONajizSfjusY9Y0P3tcPSqBd2fJGkyPAOVJKmBASpJUgMDVJKkBgaoJEkNDFBJkhoYoJIkNTBAJUlqYIBKktTAAJUkqYEBKklSAwNUkqQGBqgkSQ0MUEmSGhigkiQ1MEAlSWpggEqS1MAAlSSpgQEqSVIDA1SSpAabj2vHSU4EngzcUFUP75edBuzUN9ka+FFV7TJk29XAT4FbgVuqatm46pQkqcXYAhQ4CTgOOGVmQVU9c2Y6yd8CP55n+ydU1Y1jq06SpDthbAFaVRckWTpsXZIAzwD+cFzvL0nSOE3rM9DfB75fVVfNsb6Ac5JcnOSI+XaU5IgkK5KsWLNmzYIXKknSMNMK0IOBU+dZv2dVPQZ4EvCiJI+fq2FVnVBVy6pq2ZIlSxa6TkmShpp4gCbZHPgT4LS52lTVdf3XG4DlwG6TqU6SpNFM4wx0H+CbVXXtsJVJ7plkq5lpYF/g8gnWJ0nSOo0tQJOcCnwF2CnJtUme1686iFmXb5PcL8mZ/ez2wBeTXApcCHyqqs4aV52SJLUY5124B8+x/LlDll0H7N9PXwM8alx1SZK0EOyJSJKkBgaoJEkNDFBJkhoYoJIkNTBAJUlqYIBKktTAAJUkqYEBKklSAwNUkqQGBqgkSQ0MUEmSGhigkiQ1MEAlSWpggEqS1MAAlSSpgQEqSVIDA1SSpAYGqCRJDQxQSZIaGKCSJDUwQCVJamCASpLUwACVJKmBASpJUgMDVJKkBgaoJEkNDFBJkhoYoJIkNTBAJUlqMLYATXJikhuSXD6w7Jgk/55kZf/af45t90vyrSRXJ3nNuGqUJKnVOM9ATwL2G7L876pql/515uyVSTYD3gU8CdgZODjJzmOsU5Kk9Ta2AK2qC4AfNmy6G3B1VV1TVb8EPgI8dUGLkyTpTprGZ6AvTnJZf4n33kPW3x/43sD8tf0ySZIWjUkH6LuBhwC7ANcDfzukTYYsq7l2mOSIJCuSrFizZs3CVClJ0jpMNECr6vtVdWtV3Qa8l+5y7WzXAg8cmH8AcN08+zyhqpZV1bIlS5YsbMGSJM1hogGa5L4Ds08DLh/S7CJgxyQPTnJX4CDg9EnUJ0nSqDYf146TnArsBWyX5Frgr4C9kuxCd0l2NfCCvu39gPdV1f5VdUuSFwNnA5sBJ1bVFeOqU5KkFmML0Ko6eMji98/R9jpg/4H5M4E7POIiSdJiYU9EkiQ1MEAlSWpggEqS1MAAlSSpgQEqSVIDA1SSpAYGqCRJDQxQSZIaGKCSJDUwQCVJamCASpLUwACVJKmBASpJUgMDVJKkBgaoJEkNDFBJkhoYoJIkNTBAJUlqYIBKktTAAJUkqYEBKklSAwNUkqQGBqgkSQ0MUEmSGhigkiQ12HzaBWg6vnvsIxZ8nzscvWrB97mpWeh/F/9NpPHxDFSSpAYGqCRJDQxQSZIaGKCSJDUYW4AmOTHJDUkuH1j21iTfTHJZkuVJtp5j29VJViVZmWTFuGqUJKnVOM9ATwL2m7XsXODhVfVI4NvAa+fZ/glVtUtVLRtTfZIkNRtbgFbVBcAPZy07p6pu6We/CjxgXO8vSdI4TfMz0MOBT8+xroBzklyc5Ij5dpLkiCQrkqxYs2bNghcpSdIwUwnQJK8DbgE+NEeTPavqMcCTgBclefxc+6qqE6pqWVUtW7JkyRiqlSTpjiYeoEkOBZ4MPKuqalibqrqu/3oDsBzYbXIVSpK0bhMN0CT7Aa8GnlJVN83R5p5JtpqZBvYFLh/WVpKkaRnnYyynAl8BdkpybZLnAccBWwHn9o+oHN+3vV+SM/tNtwe+mORS4ELgU1V11rjqlCSpxdg6k6+qg4csfv8cba8D9u+nrwEeNa66JElaCPZEJElSAwNUkqQGjgcqSfNwjFbNxTNQSZIaGKCSJDUwQCVJamCASpLUwACVJKmBASpJUgMDVJKkBgaoJEkNDFBJkhoYoJIkNTBAJUlqYIBKktTAAJUkqYEBKklSAwNUkqQGBqgkSQ0MUEmSGhigkiQ1MEAlSWowUoAmOW+UZZIkbSo2n29lkrsB9wC2S3JvIP2qXwPuN+baJElatOYNUOAFwMvpwvJifhWgPwHeNca6JEla1OYN0Kr6B+Afkrykqt45oZokSVr01nUGCkBVvTPJY4Glg9tU1SljqkuSpEVtpABN8kHgIcBK4NZ+cQEGqCRpkzRSgALLgJ2rqsZZjCRJG4pRnwO9HLjP+u48yYlJbkhy+cCybZKcm+Sq/uu959j20L7NVUkOXd/3liRpnEYN0O2AbyQ5O8npM68RtjsJ2G/WstcA51XVjsB5/fxakmwD/BWwO7Ab8FdzBa0kaTySPDfJRB9ZTHJ+kmX99JlJtp6n7QFJdh6YPzbJPo3ve2SSy5N8O8kxo2wz6iXckXY2W1VdkGTprMVPBfbqp08GzgdePavN/wDOraofAiQ5ly6IT22pQ5I2NUk2r6pb5pof0XPprkBet5C1jKqq9l9HkwOAM4Bv9O2PbihvxtXAY+ge1/xmkvdX1ffm22DUu3A/fyeKmm37qrq+3+/1SX5jSJv7A4OFX9svk6RNTpLnAK+ku3nzMuD1wInAEmANcFhVfTfJScAPgUcDlyT5Kd1z/EuBG5McAryZ7iRmS+BdVfWe/j1eBRwC3AZ8GlhBd//Lh5LcDOxRVTcPqW01cBrwhH7R/6yqq4fUcjTwTuARdNlzTFX9S5K7Ax8AdgauBO4+a9/LqurGId+DdwNPAf4gyeuBPwX+Ejijqj6WZG/gbf17XQS8sKp+0e/zZOCPgS2Ap1fVN6vqM/173q1f/ot1/buMehfuT/uiAe7a7/znVfVro2zfIEOWDb2BKckRwBEAO+yww5jKkaTpSPIw4HXAnn2QbEMXAKdU1clJDgfeQXc2BvBbwD5VdWt/KXJX4HFVdXP//+WPq+p3k2wJfCnJOcBv99vvXlU3Jdmmqn6Y5MXAK6tqxTrK/ElV7daH3N8DTx5Sy/8FPltVh/eXZS9M8hm6DntuqqpHJnkkcMko34O+vtPpA7NvN9P+bnQfIe5dVd9Ocgrwwr42gBur6jFJ/pwulJ8/8HYnAKdW1Q3rOObRPgOtqq2q6tf6193okv64UbYd4vtJ7gvQfx1W5LXAAwfmH8AclxCq6oSqWlZVy5YsWdJYkiQtWn8IfKyqbgToP9raA/hwv/6DwOMG2n+0qm4dmD994MxxX+A5SVYCXwO2BXYE9gE+UFU3DbzH+jh14Osec9SyL/Ca/r3PB+4G7AA8Hvin/n0vozu7nG3Y92A+OwHfqapv9/Mn9+8z4xP914vpzs4BSPIU4L7c8WPFoZpGY6mqT9IdUIvTgZm7ag8F/mVIm7OBfZPcu795aN9+mSRtasIcV+AGDK7/+ax1g/MBXlJVu/SvB1fVOSO+x6jvP1ctAf504L13qKorh2wzzPrWN+wq5qCZy7O3svaV2EcC51TVbaO8yaijsfzJwOvAJG9mhINJcirwFWCnJNcmeR7d9fcnJrkKeGI/T5JlSd4Ht/918Qa669YXAcc2/EUkSRuD84BnJNkWbn9K4cvAQf36ZwFfHHFfZwMvTLJFv6/fSnJP4Bzg8CT3GHgPgJ8CW42w32cOfP3KPO/9kvTXWZM8ul9+QX8MJHk4XYjNNux7MF993wSWJnloP38IMMq9PJ+kO8kbyah34f7xwPQtwGq6u2nnVVUHz7Fq7yFtVzBwHbqqTqT7kFySNllVdUWSNwKfT3Ir8HXgpcCJSY6iv4loxN29j+6S5SV9kK0BDqiqs5LsAqxI8kvgTOAv6D5HPH6+m4h6Wyb5Gt1J2Vz/77+B7jPIy/r3Xk33Wem7gQ8kuYyut7sLR/wePBf4CPDeJC8FDhxo/19JDgM+mmTmJqLjR/j+PA64CfjWCG3JxtS50LJly2rFinV91n3nfPfYRyzo/nY4etWC7m9UC30cML1j2ZhsLD9fG5MJ/Zus65LjojV4p+y0a5m0US/hPiDJ8nS9Cn0/yceTPGDcxUmStFiNegn3A3R3fD29n392v+yJ4yhKkrS4JFkOPHjW4ldX1dIplLMojBqgS6rqAwPzJyV5+TgKkiQtPlX1tGnXsNiM+hjLjUmenWSz/vVs4AfjLEySpMVs1AA9HHgG8B/A9XR3O41615ckSRudUS/hvgE4tKr+E25/BudtdMEqSdImZ9QAfeRMeELX0cHAQ7CSpA3ErkedsqDPLl781ues8xGc/tnNwed3Dqiq1XO0XUrXv+3DF6K+cRo1QO+S5N6zzkBH3VaStGm7uap2mXYRC23Uz0D/FvhykjckOZauG6m/GV9ZkqSNWZKlSb6Q5JL+9dghbR6W5MIkK5NclmTHfvmzB5a/J8lmkz+C0ccDPSXJCroO5AP8SVV9Y6yVSSOy9x5p0bt7PwoLdKOkPI1uJK4n9t3u7Ug3ksuyWdsdCfxDVX0oyV2BzZL8Dl2fu3tW1X8n+Ue6vnRPmcyh/MrIl2H7wDQ0JUnra9gl3C2A4/o+eG+lGzt0tq8Ar+t7vvtEVV3VD5S9K3BR3y/93Rk+LObY+TmmJGkaXgF8H3gU3ceJ/zW7QVV9uO+k/o+As5M8n+4q6MlV9dpJFjtM03igkiTdSb8OXN+PvXkIcIfPMZP8JnBNVb2DbpixR9INbXZgkt/o22yT5EGTK/tXPAOVpE3IKI+dTMg/Ah9P8nTgc9xxIHDoPut8dpL/puvI59j+McrXA+ckuQvw38CLgP83obpvZ4BKksaqqu41ZNlVrD149mv75auBh/fTbwLeNGTb04DTxlHr+vASriRJDQxQSZIaGKCSJDUwQCVJamCASpLUwACVJKmBj7FI0ibku8c+YkGHM9vh6FXzPleaZFu6zg8A7kPXbd+afn63qvrlQtYzSQaoJGlsquoHwC4ASY4BflZVbxtsk65T2/S9Em0wvIQrSZq4JA9NcnmS44FLgAcm+dHA+oOSvK+f3j7JJ5Ks6Icx+71p1T3IM9ANxK5HLexIPcu3WtDdrZeN6Vg03EIPMQcOM7eR2hk4rKqOTDJfHr0D+Juq+mqSpcAZ9L0VTZMBKkmaln+rqotGaLcPsFM/fBnAvZPcvapuHl9p62aASpKmZbAD+dvohiqbcbeB6bAIbzjyM1BJ0tT1NxD9Z5Id+1FWnjaw+jN0I64A0A/CPXWegUrSJmRdj51M2auBs4DvAt8AtuyXvwh4d5LD6HLrcwwE6rRMPECT7MTaw9D8JnB0Vf39QJu9gH8BvtMv+kRVHTuxIiVJC66qjhmYvpr+8ZaBZUOHKauqNcCB465vfU08QKvqW/zqmaDNgH8Hlg9p+oWqevIka5MkaVTT/gx0b7q7sCY+krgkSXfGtAP0IODUOdbtkeTSJJ9O8rC5dpDkiP7h2hVr1qyZq5kkSQtqagGa5K7AU4CPDll9CfCgqnoU8E7gk3Ptp6pOqKplVbVsyZIl4ylWkqRZpnkG+iTgkqr6/uwVVfWTqvpZP30msEWS7SZdoCRJc5lmgB7MHJdvk9yn71yYJLvR1fmDCdYmSdK8pvIcaJJ7AE8EXjCw7EiAqjqe7nblFya5BbgZOKiqFnQIHkmS7oypBGhV3QRsO2vZ8QPTxwHHTbouSZJGNe27cCVJ2iAZoJIkNTBAJUlqYIBKktTAAJUkqYEBKklSAwNUkqQGBqgkSQ0MUEmSGhigkiQ1MEAlSWpggEqS1MAAlSSpgQEqSVIDA1SSpAYGqCRJDQxQSZIaGKCSJDUwQCVJamCASpLUwACVJKmBASpJUgMDVJKkBgaoJEkNDFBJkhoYoJIkNTBAJUlqYIBKktTAAJUkqcHUAjTJ6iSrkqxMsmLI+iR5R5Krk1yW5DHTqFOSpGE2n/L7P6Gqbpxj3ZOAHfvX7sC7+6+SJE3dYr6E+1TglOp8Fdg6yX2nXZQkSTDdAC3gnCQXJzliyPr7A98bmL+2X7aWJEckWZFkxZo1a8ZUqiRJa5tmgO5ZVY+hu1T7oiSPn7U+Q7apOyyoOqGqllXVsiVLloyjTkmS7mBqAVpV1/VfbwCWA7vNanIt8MCB+QcA102mOkmS5jeVAE1yzyRbzUwD+wKXz2p2OvCc/m7c3wN+XFXXT7hUSZKGmtZduNsDy5PM1PDhqjoryZEAVXU8cCawP3A1cBNw2JRqlSTpDqYSoFV1DfCoIcuPH5gu4EWTrEuSpFEt5sdYJElatAxQSZIaGKCSJDUwQCVJamCASpLUwACVJKmBASpJUgMDVJKkBgaoJEkNDFBJkhoYoJIkNTBAJUlqYIBKktTAAJUkqYEBKklSAwNUkqQGBqgkSQ0MUEmSGhigkiQ1MEAlSWpggEqS1MAAlSSpgQEqSVIDA1SSpAYGqCRJDTafdgHjtutRpyzo/pZvtaC7kyRtoDwDlSSpgQEqSVIDA1SSpAYGqCRJDSYeoEkemORzSa5MckWSlw1ps1eSHydZ2b+OnnSdkiTNZxp34d4C/O+quiTJVsDFSc6tqm/MaveFqnryFOqTJGmdJn4GWlXXV9Ul/fRPgSuB+0+6DkmS7oypfgaaZCnwaOBrQ1bvkeTSJJ9O8rB59nFEkhVJVqxZs2ZMlUqStLapBWiSewEfB15eVT+ZtfoS4EFV9SjgncAn59pPVZ1QVcuqatmSJUvGV7AkSQOmEqBJtqALzw9V1Sdmr6+qn1TVz/rpM4Etkmw34TIlSZrTNO7CDfB+4Mqqevscbe7TtyPJbnR1/mByVUqSNL9p3IW7J3AIsCrJyn7ZXwA7AFTV8cCBwAuT3ALcDBxUVTWFWiVJGmriAVpVXwSyjjbHAcdNpiJJktafPRFJktTAAJUkqYEBKklSAwNUkqQGBqgkSQ0MUEmSGhigkiQ1MEAlSWpggEqS1MAAlSSpgQEqSVIDA1SSpAYGqCRJDQxQSZIaGKCSJDUwQCVJamCASpLUYPNpFyBtqHY96pQF3+fyrRZ8lyNZ6GOZ1nHAxnUsWtw8A5UkqYEBKklSAwNUkqQGBqgkSQ0MUEmSGhigkiQ1MEAlSWpggEqS1MAAlSSpgQEqSVIDA1SSpAZTCdAk+yX5VpKrk7xmyPotk5zWr/9akqWTr1KSpLlNPECTbAa8C3gSsDNwcJKdZzV7HvCfVfVQ4O+At0y2SkmS5jeNM9DdgKur6pqq+iXwEeCps9o8FTi5n/4YsHeSTLBGSZLmlaqa7BsmBwL7VdXz+/lDgN2r6sUDbS7v21zbz/9b3+bGIfs7Ajiin90J+NaYD2E74A51bIA2luMAj2Ux2liOAyZzLDdW1X5jfg8tsGmMBzrsTHJ2io/SpltYdQJwwp0talRJVlTVskm937hsLMcBHstitLEcB2xcx6KFNY1LuNcCDxyYfwBw3VxtkmwO/Drww4lUJ0nSCKYRoBcBOyZ5cJK7AgcBp89qczpwaD99IPDZmvS1ZkmS5jHxS7hVdUuSFwNnA5sBJ1bVFUmOBVZU1enA+4EPJrma7szzoEnXOY+JXS4es43lOMBjWYw2luOAjetYtIAmfhORJEkbA3sikiSpgQEqSVIDA3SBJNkryWOnXcf6SnJ+kmX99JlJtu5ffz7lupprSHJS/7zxBifJc5McN+06RpVkaf/c9uzl7xvSw9gGI8kB61v/Yvi90WQZoAugf9RmL2CDC9BBVbV/Vf0I2BqY9n8Ei6EGNaqq51fVN6Zdx51wAF1Xo+vDn9lNzCYZoEnumeRTSS5NcnmSZyZZneQtSS7sXw/t2z4oyXlJLuu/7tAvPynJ25N8DjgNOBJ4RZKVSX4/ydP7fV+a5IIJHtvSJN9McnJf88eS3CPJ3km+nmRVkhOTbDlk29VJtgPeDDykP5a3Tqr2WdaqIclRSS7qj+n/DNT8nH7ZpUk+OLD945N8Ock1i+lsNMmz+5+vlUnek2SzJIcl+XaSzwN7DrRd60w6yc+mUvS6bT7k5+38JMuSPKU/1pX9ABLfmVaRc3zvf5bkjf3Pz1eTbN9fSXoK8Na+7UP64/m7JBckuTLJ7yb5RJKrkvx1/xaL4fdGk1RVm9wL+FPgvQPzvw6sBl7Xzz8HOKOf/lfg0H76cOCT/fRJwBnAZv38McArB/a5Crh/P731BI9tKV2vTXv28ycCrwe+B/xWv+wU4OX99PnAsn56NV23ZUuBy6f8b3R7DcC+dI8ShO6PvjOAxwMPo+u6cbu+3TYD/zYf7dvuTNf38mL4ufud/udpi37+H+med/4usAS4K/Al4LiB4zhwYPufTfsYRvx5e+Xgz9VA238GXrSIvvfP6Wv/437Z3wCvn+N7fz7wln76ZXSdv9wX2JKu45dtF8Pvja/JvjbJM1C6cNunP+P8/ar6cb/81IGve/TTewAf7qc/CDxuYD8frapb53iPLwEnJfkzuuddJ+l7VfWlfvqfgL2B71TVt/tlJ9MF0IZi3/71deAS4LeBHYE/BD5WfR/JVTXYW9Unq+q26i4jbj/heueyN7ArcFGSlf38K4Dzq2pNdYMrnDbNAhvN/nl73OwGSV4F3FxV75poZb8y7Hv/m8Av6f4gA7iYLgTnMtPhyyrgiqq6vqp+AVzD2r2raRMxjb5wp66qvp1kV2B/4E1JzplZNdhsrs0Hpn8+z3scmWR34I+AlUl2qaof3Jm618PG9nBvgDdV1XvWWpi8lLmP9Reztl8MApxcVa+9fUFyAPC0OdrfQv8xS5LQnaEuRrP/DdaaT7I38HSm+0fbHb73AEleWVUz9d7K/P8nzvxM3cbaP1+3rWM7baQ2yTPQJPcDbqqqfwLeBjymX/XMga9f6ae/zK96QnoW8MU5dvtTYKuB93hIVX2tqo6mG8lhkn+h7pBk5gz6YOAzwNKZz3WBQ4DPz7P9WscyJYM1nA0cnuReAEnun+Q3gPOAZyTZtl++zVQqHd15wIF97TP1fh3YK8m2SbagC5oZq+nOmqAb4m+LCda6Pmb/vN3+O5LkQXSXS59RVTdPo7jeHb73fW1zafkdWAy/N5qgTTJAgUcAF/aXcl4HzNwEsGWSr9F9xvGKftlLgcOSXEYXPC+bY5//Cjxt5iYiuhsQVqW7xf8C4NIxHcswVwKH9jVvQzco+WHAR5OsovuL+fi5Nu7PlL/U3wQ1lZshBmsAnkh3Gf0rff0fA7aqqiuANwKfT3Ip8PZp1Dqq/nLy64Fz+n+bc+k+RzuG7g+2z9Bdop7xXuAPklwI7M48VzymbPbP27sH1j2X7vPB5f3vxplTqG++7/1cPgIc1d9495AR32PqvzeaLLvy6yVZTXfTwwY9hmGSpXQ3QD18yqVI0kZtUz0DlSTpTvEMVJKkBp6BSpLUwACVJKmBASpJUgMDVLoT0jBqh6SNgwEq3Tkto3aslyST7gpS0ggMUC1K6UaVuTLJe5NckeScJHfvR8Y4K8nFSb6Q5Lf7UTWuSWfrJLcleXy/ny8keWiSPxgYFeTrSebsMSbJq/pOMC5N8uZ+2Z+lGw3m0iQfTzfiyLBRO+5QX7/9Q/rRPi5Kcmz6kVX6mt/aP3y/Kskz++V7Jflckg8Dq5K8IcnLBmp8Y9+VoaRpmXZv9r58DXvRdep9C7BLP//PwLPpumTbsV+2O/DZfvosutFZngxcRNfD1JZ0nehD11PUzIgh9wI2n+N9n0TXfeM9+vmZEV62HWjz18BL+umTWHvUjrnqOwM4uJ8+kn5kFbqRgc6lG3Bge7qRWe5LN77sz4EHD3w/Lumn7wL822BNvnz5mvzLDpC1mH2nqlb20zMjZTyWrkvCmTYz45p+ga6z8gcDbwL+jK6/34v69V8C3p7kQ8AnquraOd5zH+ADVXUTrDXCy8PTjfu4NV0Anz17w76v3rnq24Puci903RK+rZ9+HHBqdaP6fD/dmKC/C/wEuLCqvtPXsTrJD5I8mi5ov16TG5xA0hAGqBazwREvbqULjh9V1S5D2n6B7szufsDRwFF0Z3EXAFTVm5N8im4Enq8m2aeqvjlkP2H4CC8nAQdU1aVJntvve7a7zFPfXOYbKWZ237fvo+tb9j50425KmiI/A9WG5CfAd5I8HW7//PBR/bqv0Z393VZV/wWsBF5AF6wzo+Osqqq3ACvoxhQd5hy6kV/u0W83M8LLVsD1/Ygpzxpof/sIHFU1X31fpbtcC78a3Qe6gH9m/znuErqz6AvnqG05sB/dGeodzoAlTZYBqg3Ns4Dn9aOvXEE3zBfVDWz8Pbqggi44t6Ib/Bjg5f2NOpcCNwOfHrbzqjqLbuDkFf1oPa/sV/0lXUifCwyeuc4etWNofcDLgf/Vj6xyX2BmEPflwGV0o/V8FnhVVf3HHLX9Evgc8M8190DukibEvnClCejPaG+uqkpyEN0NRU9d13az9nEXuuHOnl5VV42jTkmj8zNQaTJ2BY5Ld3fRj4DD12fjvrOGM4Dlhqe0OHgGqk1SkkcAH5y1+BdVtfs06pG04TFAJUlq4E1EkiQ1MEAlSWpggEqS1MAAlSSpgQEqSVKD/w9asubr20I5KgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 463.875x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "review_test = news_test_data\n",
    "review_test['actual'] = news_test_labels\n",
    "review_test['predicted'] = news_pred\n",
    "review_test[\"correct_prediction?\"] = review_test.actual == review_test.predicted\n",
    "sns.catplot(x=\"news_category\", kind = \"count\", hue=\"correct_prediction?\", data = review_test)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
